---
title: "DSLabScript"
author: "Alessandro Risaro, Alessandro Maccario, Andrea Afify, Daniele Mingolla"
date: "7/7/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Caricamento delle librerie utili all' analisi
```{r, message=FALSE}
library(stR)
library(fpp2)
library(psych)
library(stats)
library(olsrr)
library(readxl)
library(lmtest)
library(forecast)
library(ggfortify)
library(lubridate)
library(systemfit)
library(skedastic)
library(DataCombine)
library(dplyr)
library(missForest)
library(imputeTS)
library(MLmetrics)
library(xts)
library(stringr)
library(missForest)
library(tibble)
library(Rcpp)
library(reticulate)
```
Pulizia enviroment e settaggio working directory
```{r}
# pulizia variabili
rm(list=ls())
# pulizia console
cat("\014")

setwd("C:/Users/39345/Desktop/Data Science/Data Science Lab/Progetto/CodiceDati")

```
## Estrazione dati meteo (python)

```{r, eval=FALSE, echo=TRUE,message=FALSE}



from datetime import datetime
import pandas as pd
import matplotlib.pyplot as plt
from meteostat import Point, Hourly,Daily
import numpy as np
start = datetime(2017, 12, 1)
end = datetime(2021, 1, 31)
location = Point(45.52283514146085, 9.212678049727053)
data_hour = Hourly(location, start, end)
data_hour = data_hour.fetch()
data_hour.reset_index(inplace=True)
data_hour['time']=pd.to_datetime(data_hour['time'])
data_hour.dtypes
data_hour.to_csv('meteo.csv')

```

## Pre processing U1 e u6 (python)
```{r,eval=FALSE,echo=TRUE,message=FALSE}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.dates import DateFormatter
import matplotlib.ticker as ticker
import seaborn as sns
import datetime
from datetime import timedelta
### PATH DATI
# !!! MODIFICARE IN BASE AL PROPRIO DRIVE

path_u1 = "/content/drive/MyDrive/DS LAB/DATA/U1_completo.csv"
path_u6 = '/content/drive/MyDrive/DS LAB/DATA/U6_completo.csv'

################ U1 #################################################
data_u1 = pd.read_csv(path_u1)
data_u1.dropna(subset = ['POD'], inplace=True)
data_u1['DATA'] = data_u1['DATA'].astype('int')
data_u1.shape
data_u1['DATA'] = pd.to_datetime(data_u1['DATA'], format = '%Y%m%d').dt.strftime('%Y-%m-%d')
data_u1.head()
# eliminiamo i duplicati e ricordiamoci che il numero di righe non è perfettamente 105.216 in quanto mancheranno degli slot orari
data_u1.drop_duplicates(subset=['DATA','ORA'], keep = 'first', inplace=True)
# funzione per convertire l'ora nel formato HH:MM
list_hours = data_u1.ORA.unique()
dict_hours = {}
for time in list_hours:
  value = str(time/10000)
  if len(value) == 3:
    value = f"0{value}0"
  elif len(value) == 4:
    splitted_str = value.split('.')
    hour = splitted_str[0]
    minutes = splitted_str[1]
    if len(hour) == 1:
      hour = f"0{hour}"
    if len(minutes) == 1:
      minutes = f"{minutes}0"
    value = ':'.join([hour,minutes])
  value = str(value).replace('.',':')
  dict_hours[time] = value
  ## mappare ore da minuti a HH:MM
data_u1['ORA'] = data_u1['ORA'].replace(dict_hours)
data_u1['DATA'] = pd.to_datetime(data_u1['DATA'], format = '%Y-%m-%d').dt.date
# CONVERSIONE DA ORA LEGALE A SOLARE
data_u1['DATA'] = data_u1['DATA'].astype('str') + ' ' + data_u1['ORA']
data_u1['DATA'] = pd.to_datetime(data_u1['DATA'])
# sottraggo un'ora
data_u1.loc[data_u1['FL_ORA_LEGALE'] == 2, 'DATA'] = data_u1['DATA'] - timedelta(hours=1)
data_u1['ORA'] = data_u1['DATA'].dt.time
data_u1.head()

################################## U6 #################################################################
data_u6 = pd.read_csv(path_u6)
data_u6.dropna(subset = ['POD', 'DATA'], inplace=True)
data_u6['DATA'] = data_u6['DATA'].astype('int')
data_u6['CONSUMO_ATTIVA_PRELEVATA'] = data_u6['CONSUMO_ATTIVA_PRELEVATA'].astype(str).str.replace(',','.')
data_u6['CONSUMO_ATTIVA_PRELEVATA'] = pd.to_numeric(data_u6['CONSUMO_ATTIVA_PRELEVATA'], errors='coerce')
data_u6['CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA'] = data_u6['CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA'].astype(str).str.replace(',','.')
data_u6['CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA'] = pd.to_numeric(data_u6['CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA'], errors='coerce')
data_u6['POTENZA_MASSIMA'] = pd.to_numeric(data_u6['POTENZA_MASSIMA'], errors='coerce')
print(data_u6.dtypes)
data_u6.shape
data_u6['DATA'] = pd.to_datetime(data_u6['DATA'], format = '%Y%m%d').dt.strftime('%Y-%m-%d')
data_u6.drop_duplicates(subset=['DATA','ORA'], keep = 'first', inplace=True)
data_u6['ORA'] = data_u6['ORA'].replace(dict_hours)
data_u6.head()
data_u6['DATA'] = pd.to_datetime(data_u6['DATA'], format = '%Y-%m-%d').dt.date
# CONVERSIONE DA ORA LEGALE A SOLARE
data_u6['DATA'] = data_u6['DATA'].astype('str') + ' ' + data_u6['ORA']
data_u6['DATA'] = pd.to_datetime(data_u6['DATA'])
# sottraggo un'ora
data_u6.loc[data_u6['FL_ORA_LEGALE'] == 2, 'DATA'] = data_u6['DATA'] - timedelta(hours=1)
data_u6['ORA'] = data_u6['DATA'].dt.time
data_u6.head()
```

## U1
Importazione dataset u1 (già pulito), statistiche descrittive e alcuni plot
```{r}
data_u1 <- read.csv("U1_DATECOMPLETE_CLEANED.csv")
data_u1$CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA_y <- as.numeric(gsub(",", ".", data_u1$CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA_y))
summary(data_u1)



######## ANALISI DESCRITTIVE - 15 MIN #######
ts_15_min = ts(data_u1$CONSUMO_ATTIVA_PRELEVATA_y,
               frequency = 365, start = c(2018, 1), end = c(2020,365))

# Grafico consumo attiva prelevata
plot(ts_15_min, 
     col="yellow", 
     xlab = "Data", 
     ylab = "kw", 
     main="U1 - Consumo attivo prelevata", 
     cex.main=0.8, 
     type='l')
# Stagionalità ogni 15 min
ggseasonplot(ts_15_min, year.labels=TRUE, year.labels.left=TRUE)+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x=element_blank()) +
  ylab("Consumo attiva prelevata") +
  xlab("15 min frequenza") +
  ggtitle("U1 - Stagionalità: C.A.P ogni 15 min")
```
Decomposizione serie temporale
```{r}
# Decomposizione
decomp <- stl(ts_15_min, s.window = "periodic", na.action = na.omit)
plot(decomp)

# Verifica residui ARIMA
# notiamo presenza correlazione, informazioni utili non utilizzate dal modello
checkresiduals(arima(ts_15_min))

```
Scelta del migliore metodo di sostituzione dei valori nulli e degli zeri (considerati come valori mancanti)
```{r}
#SOSTITUZIONE DEGLI ZERO CON NA
data_u1[data_u1 == 0] <- NA

statsNA(data_u1$CONSUMO_ATTIVA_PRELEVATA_y)

ggplot_na_distribution(data_u1$CONSUMO_ATTIVA_PRELEVATA_y)

set.seed(1)

# eliminiamo i null dal vettore colonna
db_senza_nulli <- na.omit(data_u1$CONSUMO_ATTIVA_PRELEVATA_y)
db_senza_nulli <- as.data.frame(db_senza_nulli)

# generiamo dei null artificialmente
db_test_nulli <- prodNA(db_senza_nulli, noNA = 0.00223)

# test dei vari algoritmi di imputazione
imputation <- na_interpolation(db_test_nulli)
imputation_kalman <- na_kalman(db_test_nulli)
imputation_sea <- na_seadec(db_test_nulli, find_frequency=TRUE)
imputation_mean <- na_mean(db_test_nulli)
imputation_locf <- na_locf(db_test_nulli)
imputation_seasplit <- na_seasplit(db_test_nulli, find_frequency=TRUE)
imputation_ma <- na_ma(db_test_nulli)

# plot risultati imputazione vs dati a disposizione
ggplot_na_imputations(db_test_nulli, imputation, db_senza_nulli)
ggplot_na_imputations(db_test_nulli, imputation_sea, db_senza_nulli)
ggplot_na_imputations(db_test_nulli, imputation_kalman, db_senza_nulli)
ggplot_na_imputations(db_test_nulli, imputation_mean, db_senza_nulli)
ggplot_na_imputations(db_test_nulli, imputation_locf, db_senza_nulli)
ggplot_na_imputations(db_test_nulli, imputation_seasplit, db_senza_nulli)
ggplot_na_imputations(db_test_nulli, imputation_ma, db_senza_nulli)

# test risultati ottenuti per decretare alg. imputazione migliore

# otteniamo indici dei valori nulli che sono stati imputati
index_null <- which(is.na(db_test_nulli), arr.ind = TRUE)
index_null <- as.vector(index_null)

# inseriamo valori imputati e reali nello stesso dataframe
db_for_evaluation <- cbind(imputation[index_null,],
                           imputation_sea[index_null,],
                           imputation_kalman[index_null,],
                           imputation_mean[index_null,],
                           imputation_locf[index_null,],
                           imputation_seasplit[index_null,],
                           imputation_ma[index_null,],
                           db_senza_nulli[index_null,])
db_for_evaluation <- as.data.frame(db_for_evaluation)
names(db_for_evaluation) <- c("interpolation", 
                              "seadec", 
                              "kalman", 
                              "mean",
                              "locf",
                              "seasplit",
                              "ma",
                              "original")
# TEST MSE
MSE(db_for_evaluation$interpolation, db_for_evaluation$original)
MSE(db_for_evaluation$seadec, db_for_evaluation$original)
MSE(db_for_evaluation$kalman, db_for_evaluation$original)
MSE(db_for_evaluation$mean, db_for_evaluation$original)
MSE(db_for_evaluation$locf, db_for_evaluation$original)
MSE(db_for_evaluation$seasplit, db_for_evaluation$original)
MSE(db_for_evaluation$ma, db_for_evaluation$original)

# TEST MAPE
MAPE(db_for_evaluation$interpolation, db_for_evaluation$original)*100
MAPE(db_for_evaluation$seadec, db_for_evaluation$original)*100
MAPE(db_for_evaluation$kalman, db_for_evaluation$original)*100
MAPE(db_for_evaluation$mean, db_for_evaluation$original)*100
MAPE(db_for_evaluation$locf, db_for_evaluation$original)*100
MAPE(db_for_evaluation$seasplit, db_for_evaluation$original)*100
MAPE(db_for_evaluation$ma, db_for_evaluation$original)*100

# TEST RMSE
RMSE(db_for_evaluation$interpolation, db_for_evaluation$original)
RMSE(db_for_evaluation$seadec, db_for_evaluation$original)
RMSE(db_for_evaluation$kalman, db_for_evaluation$original)
RMSE(db_for_evaluation$mean, db_for_evaluation$original)
RMSE(db_for_evaluation$locf, db_for_evaluation$original)
RMSE(db_for_evaluation$seasplit, db_for_evaluation$original)
RMSE(db_for_evaluation$ma, db_for_evaluation$original)

```

Dati gli indici ottenuti si decide di procedere con il metodo di imputation di kalman
```{r,warning=FALSE}
imputation_con_att <- na_kalman(data_u1$CONSUMO_ATTIVA_PRELEVATA_y) 
imputation_pot_max <- na_kalman(data_u1$POTENZA_MASSIMA) 

ggplot_na_imputations(data_u1$CONSUMO_ATTIVA_PRELEVATA_y, imputation_con_att)
#ggplot_na_imputations(data_u1$POTENZA_MASSIMA, imputation_pot_max)


data_u1[['CONSUMO_ATTIVA_PRELEVATA_y']] <- imputation_con_att
#data_u1[['POTENZA_MASSIMA_y']] <- imputation_pot_max
```
Eliminazione colonne inutili ai fini della nostra analisi
```{r}
# colonne inutili
data_u1 <- subset(data_u1, select = -c(POD_y,FL_ORA_LEGALE_y, TIPO_DATO_y,ANNO_y))
```
Parte in python di pre processing: aggregazione per ore e integrazione dei dati meteo
```{r,eval=FALSE,echo=TRUE, message=FALSE}
path_u1 = '/content/drive/MyDrive/DS LAB/DATA/U1_15min_kalman.csv'
data_u1 = pd.read_csv(path_u1)
data_u1.head()
data_u1['DATA'] = pd.to_datetime(data_u1['DATA'], format = '%Y-%m-%d %H:%M:%S')
data_u1.set_index('DATA', inplace=True)
# da 15 min raggruppo per ora
data_u1 = data_u1.resample('60min', offset=0).agg({'CONSUMO_ATTIVA_PRELEVATA_y':'sum', 
                                                           'CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA_y':'sum',
                                                           'POTENZA_MASSIMA_y': 'mean'})
data_u1.head()
dati_meteo = pd.read_csv("/content/drive/MyDrive/DS LAB/DATA/meteo_completo.csv", parse_dates = ['time'], index_col= 'time')
dati_meteo.drop(columns=['Unnamed: 0'], inplace=True)
# MERGE DATI METEO
dati_meteo = pd.read_csv("/content/drive/MyDrive/DS LAB/DATA/meteo_completo.csv", parse_dates = ['time'], index_col= 'time')
dati_meteo.drop(columns=['Unnamed: 0'], inplace=True)
# join tra dati u1 (raggruppati per ora) e dati meteo (per ora)
u1_weather = pd.merge(data_u1, dati_meteo, how='left', left_index=True, right_index=True)
u1_weather['ONLY_DATA'] = u1_weather.index.date
u1_weather.head(50)
u1_weather.to_csv("join_meteo_u1.csv")

```

Caricamento dati arricchiti in python
```{r}
# leggo i dati u1 per ore arricchiti con il meteo su python
join_meteo_u1 <- read.csv("join_meteo_u1.csv")
```
Sostituzione dei valori nulli della temperatura con kalman
```{r}
imputation_temp <- na_kalman(join_meteo_u1$temp) 
join_meteo_u1[['temp']] <- imputation_temp
```
Effettuiamo un aggregazione sui giorni
```{r}
# solo attributi x media
u1_completo_giorno_mean <- aggregate(list(temp=join_meteo_u1$temp, dwpt=join_meteo_u1$dwpt,
                                          rhum=join_meteo_u1$rhum,
                                          wdir=join_meteo_u1$wdir, wspd=join_meteo_u1$wspd,
                                          pres=join_meteo_u1$pres, kelvin=join_meteo_u1$kelvin, 
                                          POTENZA_MASSIMA=join_meteo_u1$POTENZA_MASSIMA), 
                                     by = list(data=join_meteo_u1$ONLY_DATA), mean)

# solo attributi x somma
u1_completo_giorno_sum <- aggregate(list(CONSUMO_ATTIVA_PRELEVATA=join_meteo_u1$CONSUMO_ATTIVA_PRELEVATA_y, 
                                         CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA=join_meteo_u1$CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA_y), 
                                    by = list(data=join_meteo_u1$ONLY_DATA), sum)

# u1 completo giorno
u1_completo_giorno = merge(x=u1_completo_giorno_mean, y=u1_completo_giorno_sum, by="data")

```

Analisi descrittive sul dataset aggregato per giorno
```{r}
######## ANALISI DESCRITTIVE X GIORNO ########

ts_giorno = ts(u1_completo_giorno$CONSUMO_ATTIVA_PRELEVATA,
               frequency = 365, start = c(2018,1))

######## ACF
acf(ts_giorno, lag.max = 30)


# Grafico x giorno
plot(ts_giorno, 
     col="red",
     ylab = "C.A.P",
     xlab = "Giorni",
     main="U1 - C.A.P - Aggregata per giorno",
     cex.main=0.8,
     type='l')

# Decomposizione 
ts_giorno_na_omit = ts(na.omit(u1_completo_giorno$CONSUMO_ATTIVA_PRELEVATA),
                       frequency = 365, start = c(2018,1))
decomp <- stl(ts_giorno_na_omit, s.window = "periodic", na.action = na.omit)
plot(decomp)

# C.A.P grafico per mese
ggmonthplot(ts_giorno) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x=element_blank()) +
  ylab("C.A.P") +
  xlab("Giorno") + 
  ggtitle("Subseries: C.A.P per giorno")

# Stagionalità x mese
ggseasonplot(ts_giorno, year.labels=TRUE, year.labels.left=TRUE)+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x=element_blank()) +
  ylab("C.A.P") +
  ggtitle("Stagionalità: C.A.P per giorno")

```
#Cross correlation - U1
```{r}
corr_temp_cons = Ccf(u1_completo_giorno$temp, u1_completo_giorno$CONSUMO_ATTIVA_PRELEVATA)
corr_cons_temp = Ccf(as.ts(u1_completo_giorno$CONSUMO_ATTIVA_PRELEVATA), as.ts(u1_completo_giorno$temp))
```
# Analisi predittiva con TBATS consumi 2021 - U1    
Eseguiamo prima una prova utilizzando un training set e un validation set
```{r}
###### PREDIZIONE TBATS - TEST 2019 #######

ts_u1_att_pre <- ts(u1_completo_giorno$CONSUMO_ATTIVA_PRELEVATA,
                    start = c(2018,1),
                    end = c(2021,1),
                    frequency = 365)

training=window(ts_u1_att_pre, start = c(2018,1), end = c(2018,365))
validation = window(ts_u1_att_pre, start = c(2019, 1), end = c(2019,365))

pred.train <- tbats(training, seasonal.periods = c(7,365))
pred.train %>%
  # H = numero di giorni in avanti su cui prevedere
  forecast(h=365) %>%
  autoplot() + autolayer(validation)

## CALC MAPE
tbats_model = tbats(training, seasonal.periods = c(7, 365))
tbats_forecast = forecast(tbats_model, h=365)
MAPE(tbats_forecast$mean, validation) * 100
```
Svolgiamo ora la vera analisi predittiva sull'anno 2021
```{r}

# nell'end bisogna inserire il mese successivo rispetto a quello
# in cui termina realmente la serie
ts_u1_att_pre <- ts(u1_completo_giorno$CONSUMO_ATTIVA_PRELEVATA,
            start = c(2018,1),
            end = c(2021,1),
            frequency = 365)

# inserire mese successivo rispetto a quello in cui termina la serie
training=window(ts_u1_att_pre, start = c(2018,1), end = c(2020,366))

# per farlo funzionare mettere 24,7,365 se ho le ore
# per farlo funzionare mettere 7,365 se ho raggruppato per giorni
pred.train <- tbats(training, seasonal.periods = c(7,365))
pred.train %>%
# H = numero di giorni in avanti su cui prevedere
  forecast(h=365) %>%
  autoplot()

```
# Prophet - U1 (Python)
```{r,eval=FALSE,echo=TRUE,message=FALSE}
data = pd.read_csv("u1_completo_giorno.csv", sep=",")
data.set_index('Data', inplace=True)
data_ph = pd.DataFrame()
data_ph = data[['CONSUMO_ATTIVA_PRELEVATA']].copy()
data_ph.reset_index(inplace=True)
data_ph.rename(columns={"Data": "ds", "CONSUMO_ATTIVA_PRELEVATA": "y"}, inplace=True)
data_ph['ds'] = pd.to_datetime(data_ph['ds'], format='%Y-%m-%d')
data_ph.head()


threshold = pd.to_datetime("2018-12-31", format="%Y-%m-%d")

train = data_ph[data_ph['ds'] <= threshold]
validation = data_ph[(data_ph['ds'] > threshold) & (data_ph['ds'] < "2020-01-01")]
model = Prophet(yearly_seasonality=True)
model.add_seasonality(name='monthly', period=30.5, fourier_order=5)
future = model.fit(train).make_future_dataframe(periods=365)
predict = model.predict(future)
fig1 = model.plot(predict)


validation['yhat'] = predict[(predict['ds']> threshold) & (predict['ds'] < "2020-01-01")]['yhat']
validation.set_index('ds', inplace=True)
validation.plot()


# mape
np.mean(np.abs((validation['y'] - validation['yhat']) / validation['y'])) * 100
```

# Analisi impatto covid - U1 
```{r}
## 2020, 53 sarebbe fino al 22 febbraio circa del 2020
training=window(ts_u1_att_pre, start = c(2018,1), end = c(2020,53))
validation=window(ts_u1_att_pre, start = c(2020,54), end= c(2020, 254))

pred.train <- tbats(training, seasonal.periods = c(7,30,365))
pred.train %>%
  forecast(h=length(validation)) %>%
  autoplot() + autolayer(validation)
```
## U6
Importazione dataset u6 (già pulito), statistiche descrittive e alcuni plot
```{r,message=FALSE}

u6_compl <- read.csv("U6_DATECOMPLETE_CLEANED.csv")

u6_compl$DATA <-as.POSIXct(u6_compl$DATA, format='%d/%m/%Y %H:%M')
summary(u6_compl)

######## ANALISI DESCRITTIVE - 15 MIN #######
######## ANALISI DESCRITTIVE - 15 MIN #######
ts_15_min = ts(u6_compl$CONSUMO_ATTIVA_PRELEVATA,
               frequency = 365, start = c(2018, 1), end = c(2020,365))


# Grafico consumo attiva prelevata
plot(ts_15_min, 
     col="yellow", 
     xlab = "Data", 
     ylab = "kw", 
     main="U6 - Consumo attivo prelevata", 
     cex.main=0.8, 
     type='l')
# Stagionalità ogni 15 min
ggseasonplot(ts_15_min, year.labels=TRUE, year.labels.left=TRUE)+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x=element_blank()) +  # to center the plot title
  ylab("Consumo attiva prelevata") +
  xlab("15 min frequenza") +
  ggtitle("Stagionalità: C.A.P ogni 15 min")
```
Decomposizione serie temporale
```{r}
# Decomposizione
decomp <- stl(ts_15_min, s.window = "periodic", na.action = na.omit)
plot(decomp)

# Verifica residui ARIMA
# notiamo presenza correlazione, informazioni utili non utilizzate dal modello
checkresiduals(arima(ts_15_min))

```
Scelta del migliore metodo di sostituzione dei valori nulli e degli zeri (considerati come valori mancanti)
```{r}
# dividiamo dati per giugno e suo complementare
u6_compl_no_giugno <- subset(u6_compl, DATA < "2020-06-01 00:00" | DATA > "2020-06-30 23:45")
u6_compl_giugno <- subset(u6_compl, DATA < "2020-07-01 00:00" & DATA > "2020-05-31 23:45")

# convertiamo gli 0 in NA
u6_compl_no_giugno[u6_compl_no_giugno == 0] <- NA
statsNA(u6_compl_no_giugno$CONSUMO_ATTIVA_PRELEVATA)

#### TEST IMPUTAZIONE SOLO SU "CONS. ATT. PREL."

# considero solo i NAN che non sono di Giugno
no_giugno_NAN <- u6_compl_no_giugno$CONSUMO_ATTIVA_PRELEVATA
no_giugno_NAN <- as.data.frame(no_giugno_NAN)

# elimino NAN per poi usare tale dataset per la verifica dell'imputazione
no_giugno_NAN_test <- na.omit(no_giugno_NAN)

# produzione artificiale dei NAN
no_giugno_NAN_train <- prodNA(no_giugno_NAN_test, noNA = 0.00202)

# test algoritmi imputazione
imputation <- na_interpolation(no_giugno_NAN_train)
imputation_2 <- na_seadec(no_giugno_NAN_train, find_frequency=TRUE)
imputation_3 <- na_kalman(no_giugno_NAN_train)
imputation_4 <- na_mean(no_giugno_NAN_train)
imputation_5 <- na_locf(no_giugno_NAN_train)
imputation_6 <- na_seasplit(no_giugno_NAN_train, find_frequency=TRUE)
imputation_7 <- na_ma(no_giugno_NAN_train)

# grafici x ogni imputazione
ggplot_na_imputations(no_giugno_NAN_train, imputation, no_giugno_NAN_test)
ggplot_na_imputations(no_giugno_NAN_train, imputation_2, no_giugno_NAN_test)
ggplot_na_imputations(no_giugno_NAN_train, imputation_3, no_giugno_NAN_test)
ggplot_na_imputations(no_giugno_NAN_train, imputation_4, no_giugno_NAN_test)
ggplot_na_imputations(no_giugno_NAN_train, imputation_5, no_giugno_NAN_test)
ggplot_na_imputations(no_giugno_NAN_train, imputation_6, no_giugno_NAN_test)
ggplot_na_imputations(no_giugno_NAN_train, imputation_7, no_giugno_NAN_test)

# indici dei valori imputati
indici_nulli <- which(is.na(no_giugno_NAN_train), arr.ind = TRUE)
indici_nulli <- as.data.frame(indici_nulli)
riga <- indici_nulli$row
riga <- as.vector(riga)

# creo DF per testare il grado di correttezza imputazioni
db_test_imputation <- cbind(imputation[riga,],
                 imputation_2[riga,],
                 imputation_3[riga,],
                 imputation_4[riga,],
                 imputation_5[riga,],
                 imputation_6[riga,],
                 imputation_7[riga,],
                 no_giugno_NAN_test[riga,])

db_test_imputation <- as.data.frame(db_test_imputation)

names(db_test_imputation) <- c("interpolation", 
                    "seadec", 
                    "kalman", 
                    "mean",
                    "locf",
                    "seasplit",
                    "ma",
                    "original")

# calcolo del MSE, del MAPE, del RMSE e del R2_Score
MSE(db_test_imputation$interpolation, db_test_imputation$original)
MSE(db_test_imputation$seadec, db_test_imputation$original)
MSE(db_test_imputation$kalman, db_test_imputation$original)
MSE(db_test_imputation$mean, db_test_imputation$original)
MSE(db_test_imputation$locf, db_test_imputation$original)
MSE(db_test_imputation$seasplit, db_test_imputation$original)
MSE(db_test_imputation$ma, db_test_imputation$original)

MAPE(db_test_imputation$interpolation, db_test_imputation$original)*100
MAPE(db_test_imputation$seadec, db_test_imputation$original)*100
MAPE(db_test_imputation$kalman, db_test_imputation$original)*100
MAPE(db_test_imputation$mean, db_test_imputation$original)*100
MAPE(db_test_imputation$locf, db_test_imputation$original)*100
MAPE(db_test_imputation$seasplit, db_test_imputation$original)*100
MAPE(db_test_imputation$ma, db_test_imputation$original)*100

RMSE(db_test_imputation$interpolation, db_test_imputation$original)
RMSE(db_test_imputation$seadec, db_test_imputation$original)
RMSE(db_test_imputation$kalman, db_test_imputation$original)
RMSE(db_test_imputation$mean, db_test_imputation$original)
RMSE(db_test_imputation$locf, db_test_imputation$original)
RMSE(db_test_imputation$seasplit, db_test_imputation$original)
RMSE(db_test_imputation$ma, db_test_imputation$original)

# R^2
R2_Score(db_test_imputation$interpolation, db_test_imputation$original)
R2_Score(db_test_imputation$seadec, db_test_imputation$original)
R2_Score(db_test_imputation$kalman, db_test_imputation$original)
R2_Score(db_test_imputation$mean, db_test_imputation$original)
R2_Score(db_test_imputation$locf, db_test_imputation$original)
R2_Score(db_test_imputation$seasplit, db_test_imputation$original)
R2_Score(db_test_imputation$ma, db_test_imputation$original)

# dai valori ottenuti notiamo come in media Kalman restituisca un risultato migliore


```

Dati gli indici ottenuti si decide di procedere con il metodo di imputation di kalman
```{r,warning=FALSE}
# imputiamo i valori nulli (tranne di Giugno) con Kalman
u6_inserimento_val_nulli <- na_kalman(u6_compl_no_giugno)



```
Sistemiamo il dataset
```{r}
# imputiamo i valori di u6_compl_giugno come NA
u6_compl_giugno$CONSUMO_ATTIVA_PRELEVATA[u6_compl_giugno$CONSUMO_ATTIVA_PRELEVATA != -1] <- NA
u6_compl_giugno$CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA[u6_compl_giugno$CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA != -1] <- NA
u6_compl_giugno$POTENZA_MASSIMA[u6_compl_giugno$POTENZA_MASSIMA != -1] <- NA

# uniamo dataset imputato con kalman (senza giugno)
# al dataset contenente solo giugno (con tutti i val null)
u6_for_tbats <- rbind(u6_inserimento_val_nulli, u6_compl_giugno)

# ordiniamo per date
u6_for_tbats <- u6_for_tbats[order(u6_for_tbats$DATA), ]

```
Aggregazione per giorno e integrazione dei dati meteo
```{r}
######### ARRICCHIMENTO DI U6 CON I DATI METEO

# CREIAMO IL DATASET DI U6 AGGREGATO PER ORE
u6_for_tbats$SOLO_ORA <- hour(u6_for_tbats$DATA)
u6_for_tbats$SOLO_DATA <-format(u6_for_tbats$DATA, format= "%Y-%m-%d")

# creiamo un'altra colonna concatenando data e ora per poi aggregare
u6_for_tbats$CONCAT_DATA_ORA <- paste(u6_for_tbats$SOLO_DATA,
                                      u6_for_tbats$SOLO_ORA)

u6_for_tbats_ora = aggregate(u6_for_tbats$CONSUMO_ATTIVA_PRELEVATA,
                                  by=list(u6_for_tbats$CONCAT_DATA_ORA), 
                                  sum)

colnames(u6_for_tbats_ora)[1] <- "Data"
colnames(u6_for_tbats_ora)[2] <- "CONSUMO_ATTIVA_PRELEVATA"


u6_for_tbats_ora_2 = aggregate(u6_for_tbats$CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA,
                                    by=list(u6_for_tbats$CONCAT_DATA_ORA), 
                                    sum)

colnames(u6_for_tbats_ora_2)[1] <- "Data"
colnames(u6_for_tbats_ora_2)[2] <- "CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA"


u6_for_tbats_ora_3 = aggregate(u6_for_tbats$POTENZA_MASSIMA,
                                    by=list(u6_for_tbats$CONCAT_DATA_ORA), 
                                    mean)

colnames(u6_for_tbats_ora_3)[1] <- "Data"
colnames(u6_for_tbats_ora_3)[2] <- "POTENZA_MASSIMA"


# creiamo un unico dataset
u6_ora_meteo <- cbind(u6_for_tbats_ora,
                      u6_for_tbats_ora_2,
                      u6_for_tbats_ora_3)

# elimino colonne superflue
u6_ora_meteo <- subset(u6_ora_meteo, select = -c(3,5))

u6_ora_meteo$Data <- as.POSIXct(u6_ora_meteo$Data, format='%Y-%m-%d %H')

# leggo dati meteo
dati_meteo <- read.csv("meteo_completo.csv")
dati_meteo$data <- as.POSIXct(dati_meteo$data, format='%d/%m/%Y %H:%M')

# fix bug colonna kelvin
dati_meteo$kelvin <- dati_meteo$temp + 273.15

# colonna con stesso nome per svolgere join
colnames(u6_ora_meteo)[1] <- "data"
u6_meteo <- merge(x=u6_ora_meteo, y=dati_meteo, by="data",all.x=TRUE)

# elimino colonne superflue
u6_meteo <- subset(u6_meteo, select = -c(9,8,14,12))

# creiamo dataset per giugno e complementare, poi imputiamo anche i dati meteo sul complementare
u6_meteo_no_giugno <- subset(u6_meteo, data < "2020-06-01 00:00" | data > "2020-06-30 23:00")
u6_meteo_giugno <- subset(u6_meteo, data < "2020-07-01 00:00" & data > "2020-05-31 23:00")

# imputiamo TUTTI i valori nulli
u6_meteo_no_giugno <- na_kalman(u6_meteo_no_giugno)

# otteniamo dataset completo x ore
u6_completo_ore <- rbind(u6_meteo_no_giugno, u6_meteo_giugno)

ts_u6_completo_ore <- ts(u6_completo_ore$CONSUMO_ATTIVA_PRELEVATA,
                            start = c(2018,1),
                            frequency = 24)

# AGGREGHIAMO X GIORNO
u6_completo_ore$SOLO_DATA <- format(u6_completo_ore$data, format= "%Y-%m-%d")

# solo attributi x media
u6_completo_giorno_mean <- aggregate(list(temp=u6_completo_ore$temp, dwpt=u6_completo_ore$dwpt,
                                          rhum=u6_completo_ore$rhum,
                                     wdir=u6_completo_ore$wdir, wspd=u6_completo_ore$wspd,
                                     pres=u6_completo_ore$pres, kelvin=u6_completo_ore$kelvin, 
                                     POTENZA_MASSIMA=u6_completo_ore$POTENZA_MASSIMA), 
                                     by = list(data=u6_completo_ore$SOLO_DATA), mean)

# solo attributi x somma
u6_completo_giorno_sum <- aggregate(list(CONSUMO_ATTIVA_PRELEVATA=u6_completo_ore$CONSUMO_ATTIVA_PRELEVATA, 
                                         CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA=u6_completo_ore$CONSUMO_REATTIVA_INDUTTIVA_PRELEVATA), 
                                     by = list(data=u6_completo_ore$SOLO_DATA), sum)

# u6 completo giorno
u6_completo_giorno = merge(x=u6_completo_giorno_mean, y=u6_completo_giorno_sum, by="data")
u6_completo_giorno$data <- as.POSIXct(u6_completo_giorno$data, format='%Y-%m-%d')

```


## Sostituzione valori mancanti Giugno 2020
Facciamo un test per vedere come performa il validation per la predizione di maggio 2020
```{r}
########## VALIDAZIONE DEL TBATS SU MAGGIO 2020

ts_u6_completo_giorno <- ts(u6_completo_giorno$CONSUMO_ATTIVA_PRELEVATA,
                       start = c(2018,1),
                       end = c(2021,1),
                       frequency = 365)

# TRAINING -> PERIODO PRIMA DI MAGGIO
# VALIDATION -> MESE DI MAGGIO
training = window(ts_u6_completo_giorno, start = c(2018,1), end = c(2020,121))
validation = window(ts_u6_completo_giorno, start = c(2020,122), end = c(2020,152))

# prevedo Maggio (H=31)
pred.train <- tbats(training, seasonal.periods = c(7, 30, 365))
pred.train %>%
  forecast(h=31) %>%
  autoplot() + autolayer(validation)

# Controllo sul valore del MAPE
tbats_model = tbats(training, seasonal.periods = c(7,30, 365))
tbats_forecast = forecast(tbats_model, h=31)


MAPE(tbats_forecast$mean, validation) * 100

```
Facciamo ora la previsione dei dati su Giugno 2020
```{r}
##### PREVISIONI SU GIUGNO (USATI X SOSTITUIRE I NAN IN GIUGNO 2020)

# TRAINING -> FINO A MAGGIO
training <- window(ts_u6_completo_giorno, start = c(2018,1), end = c(2020,152))
pred.train <- tbats(training, seasonal.periods = c(7, 30, 365))

pred.train %>%
  forecast(h=30) %>%
  autoplot()

tbats_model = tbats(training, seasonal.periods = c(7,30, 365))
tbats_forecast = forecast(tbats_model, h=30)

# valori previsti dal TBATS da sostituire a Giugno 2020
tbats_forecast$mean
valori_forecasting <- as.data.frame(tbats_forecast$mean)

# sostituisco i valori del mese di Giugno 2020
u6_completo_giorno[(u6_completo_giorno$data<"2020-07-01" & u6_completo_giorno$data>"2020-05-31"), "CONSUMO_ATTIVA_PRELEVATA"] <- tbats_forecast$mean

# write.table(u6_completo_giorno, "./u6_completo_giorno.csv",sep=";", row.names = FALSE)

```
Svolgiamo alcune analisi descrittive
```{r,message=FALSE}
####### ANALISI DESCRITTIVE X MESE ########
u6_completo_mese = u6_completo_giorno %>%
  mutate(mese = format(data, "%m"), anno = format(data, "%Y")) %>%
  group_by(mese, anno) %>%
  summarise(CONSUMO_ATTIVA_PRELEVATA = mean(CONSUMO_ATTIVA_PRELEVATA))

u6_completo_mese <- u6_completo_mese[order(u6_completo_mese$anno), ]

ts_mese = ts(u6_completo_mese$CONSUMO_ATTIVA_PRELEVATA,
             frequency = 12, start = c(2018,1))

# Grafico x mese
plot(ts_mese, 
     col="red",
     ylab = "C.A.P",
     xlab = "Mesi",
     main="C.A.P - Aggregata per mese",
     cex.main=0.8,
     type='l')

# Decomposizione 
ts_mese_na_omit = ts(na.omit(u6_completo_mese$CONSUMO_ATTIVA_PRELEVATA),
             frequency = 12, start = c(2018,1))
decomp <- stl(ts_mese_na_omit, s.window = "periodic", na.action = na.omit)
plot(decomp)

# C.A.P grafico per mese
ggmonthplot(ts_mese) +
  ylab("C.A.P") +
  xlab("Mese") + 
  ggtitle("Subseries: C.A.P per mese")

# Stagionalità x mese
ggseasonplot(ts_mese, year.labels=TRUE, year.labels.left=TRUE)+
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("C.A.P") +
  ggtitle("Stagionalità: C.A.P per mese")

######## ANALISI DESCRITTIVE X GIORNO ########

ts_giorno = ts(u6_completo_giorno$CONSUMO_ATTIVA_PRELEVATA,
             frequency = 365, start = c(2018,1))


acf(ts_u6_completo_ore, lag.max = 96, na.action = na.pass)

# Grafico x giorno
plot(ts_giorno, 
     col="red",
     ylab = "C.A.P",
     xlab = "Giorni",
     main="U6 - C.A.P - Aggregata per giorno",
     cex.main=0.8,
     type='l')

# Decomposizione 
ts_giorno_na_omit = ts(na.omit(u6_completo_giorno$CONSUMO_ATTIVA_PRELEVATA),
                     frequency = 365, start = c(2018,1))
decomp <- stl(ts_giorno_na_omit, s.window = "periodic", na.action = na.omit)
plot(decomp)

# C.A.P grafico per mese
ggmonthplot(ts_giorno) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x=element_blank()) +
  ylab("C.A.P") +
  xlab("Giorno") + 
  ggtitle("Subseries: C.A.P per giorno")

# Stagionalità x mese
ggseasonplot(ts_giorno, year.labels=TRUE, year.labels.left=TRUE)+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x=element_blank()) +
  ylab("C.A.P") +
  ggtitle("Stagionalità: C.A.P per giorno")

# APPRONDIMENTO ANALISI RESIDUI --> https://otexts.com/fpp2/residuals.html

# 15 min
checkresiduals(arima(ts_15_min))
# giorno
checkresiduals(arima(ts_giorno))
# mese
checkresiduals(arima(ts_mese))


```
#Cross correlation - U6
```{r}


Ccf(u6_completo_giorno$CONSUMO_ATTIVA_PRELEVATA, u6_completo_giorno$temp)



```
# Analisi predittiva con TBATS consumi 2021 - U6    
Eseguiamo prima una prova utilizzando un training set e un validation set
```{r}
###### PREDIZIONE TBATS - TEST 2019  #######

ts_u6_att_pre <- ts(u6_completo_giorno$CONSUMO_ATTIVA_PRELEVATA,
                    start = c(2018,1),
                    end = c(2020,366),
                    frequency = 365)

training=window(ts_u6_att_pre, start = c(2018,1), end = c(2018,365))
validation = window(ts_u6_att_pre, start = c(2019, 1), end = c(2019,365))

pred.train <- tbats(training, seasonal.periods = c(7,30,365))
pred.train %>%
  forecast(h=365) %>%
  autoplot() + autolayer(validation)

## CALC MAPE
tbats_model = tbats(training, seasonal.periods = c(7,30, 365))
tbats_forecast = forecast(tbats_model, h=365)
MAPE(tbats_forecast$mean, validation) * 100

```
Svolgiamo ora la vera analisi predittiva sull'anno 2021
```{r}

###### PREDIZIONE TBATS - CONSUMO ATTIVA PRELEVATA  #######

ts_u6_att_pre <- ts(u6_completo_giorno$CONSUMO_ATTIVA_PRELEVATA,
                    start = c(2018,1),
                    end = c(2021,1),
                    frequency = 365)

training=window(ts_u6_att_pre, start = c(2018,1), end = c(2020,366))

pred.train <- tbats(training, seasonal.periods = c(7,365))
pred.train %>%
  forecast(h=365) %>%
  autoplot()

```
# Prophet - U6 (Python)
```{r,eval=FALSE,echo=TRUE,message=FALSE}
threshold = pd.to_datetime("2018-12-31", format="%Y-%m-%d")

train = data_ph[data_ph['ds'] <= threshold]
validation = data_ph[(data_ph['ds'] > threshold) & (data_ph['ds'] < "2020-01-01")]
model = Prophet(yearly_seasonality=True)
model.add_seasonality(name='monthly', period=30.5, fourier_order=5)
future = model.fit(train).make_future_dataframe(periods=365)
predict = model.predict(future)
fig1 = model.plot(predict)


validation['yhat'] = predict[(predict['ds']> threshold) & (predict['ds'] < "2020-01-01")]['yhat']
validation.set_index('ds', inplace=True)
validation.plot()


# mape
np.mean(np.abs((validation['y'] - validation['yhat']) / validation['y'])) * 100
```
# Analisi impatto covid - U6
```{r}
###### PREDIZIONE TBATS - CONSUMO ATTIVA PRELEVATA - COVID #######

training=window(ts_u6_att_pre, start = c(2018,1), end = c(2020,53))
validation=window(ts_u6_att_pre, start = c(2020,54), end= c(2020, 254))

pred.train <- tbats(training, seasonal.periods = c(7,30,365))
pred.train %>%
  forecast(h=length(validation)) %>%
  autoplot() + autolayer(validation)



```

